**2025-07-09**\
It's not really a textbook in a classical sense, just a pile of information I found and *found* interesting (not necessarily it means I *understood* it though, because I like to stare at something I don't understand).\
Ok, here's the information about switch FNN (the subset of the multi-branched neural networks, that contain different kinds of neural networks to process some information). I didn't understand what is said on the picture very well, but it looks interesting.\
![switch-ffn](./ml_diary_pictures/switch-ffn.png)\
It's an MoE (mixture of experts) approach to improve efficiency and performance, the name tells what it does pretty explicitly.\

**How softmax works.**\
The code is:
```Python
def softmax(x):
    exps = np.exp(x - np.max(x))  # Numerically stable
    return exps / np.sum(exps)
```
1) It's numerically stable, because if the numbers of the input matrix (x) are high, then e^x is very high.\
`Input matrix: [50,90,100]`\
`(x-np.max(x)): [-50,-10,0]`\
But what if:\
`Input matrix: [0,0,100]`? Then\
`(x-np.max(x)): [-100,-100,0]`,\
that doesn't imporve the situation at all.\
UPD: I asked QWEN, and it said that in case of the very low numbers [-100,-100,0] we get underflow, and it's pretty good, if we don't need the *exact* small numbers. In this case we get `exps([-100,-100,0]) ≈ [3.7e-44, 3.7e-44, 1]`, and it's practically totally fine, because it's the one-hot result.
2) In the 2nd line `exps / np.sum(exps)` makes the same array, but that sums to 1 (because of the nature of the division operation).\
To put it simply, if we have an list [1,2,3], then sum is 6, and the output list is [1/6,2/6,3/6]=[1/6,1/3,1/2], that sums up to 1, making all the output numbers effeciently the probabilites.

**2025-07-11**\
Btw, use the common slashes and `./` for the picture's directory in the .md files (where `./` is a current directory).\
Let's discuss the formula in the code:
```Python
def cross_entropy_loss(probs, label):
    return -np.log(probs[label]+1e-10)
```
It's the natural logarithm:\
![lnx](./ml_diary_pictures/lnx.png)\
That we convert into the -ln(x):\
![minuslnx](./ml_diary_pictures/minuslnx.png)\
The 2nd graph means that if the function's input (`probs[label]`) is low, we get a higher number. If the prediction is high, we get a lower number. At x=1 y=0 (almost). It's made this way, because if the prediction is close to 1 (high), then the loss should be smaller. The input probability shouldn't be higher than 1, because the input ptobability is (ideally) the output of the softmax function, and the sum of its outputs can't be higher than 1.\
Btw, the important math fact:
```math
1e-10=10^{-10}
```
but:
```math
1^{-10}=1/{1^{10}}=1
```
so:
```math
1e-10≠1^{-10}
```
So don't confuse these 2.







