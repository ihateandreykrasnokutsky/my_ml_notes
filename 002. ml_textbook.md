**2025-07-09**\
It's not really a textbook in a classical sense, just a pile of information I found and *found* interesting (not necessarily it means I *understood* it though, because I like to stare at something I don't understand).\
Ok, here's the information about switch FNN (the subset of the multi-branched neural networks, that contain different kinds of neural networks to process some information). I didn't understand what is said on the picture very well, but it looks interesting.\
![switch-ffn](./ml_diary_pictures/switch-ffn.png)\
It's an MoE (mixture of experts) approach to improve efficiency and performance, the name tells what it does pretty explicitly.\

**How softmax works.**\
The code is:
```Python
def softmax(x):
    exps = np.exp(x - np.max(x))  # Numerically stable
    return exps / np.sum(exps)
```
1) It's numerically stable, because if the numbers of the input matrix (x) are high, then e^x is very high.\
`Input matrix: [50,90,100]`\
`(x-np.max(x)): [-50,-10,0]`\
But what if:\
`Input matrix: [0,0,100]`? Then\
`(x-np.max(x)): [-100,-100,0]`,\
that doesn't imporve the situation at all.\
UPD: I asked QWEN, and it said that in case of the very low numbers [-100,-100,0] we get underflow, and it's pretty good, if we don't need the *exact* small numbers. In this case we get `exps([-100,-100,0]) ‚âà [3.7e-44, 3.7e-44, 1]`, and it's practically totally fine, because it's the one-hot result.
2) In the 2nd line `exps / np.sum(exps)` makes the same array, but that sums to 1 (because of the nature of the division operation).\
To put it simply, if we have an list [1,2,3], then sum is 6, and the output list is [1/6,2/6,3/6]=[1/6,1/3,1/2], that sums up to 1, making all the output numbers effeciently the probabilites.

**2025-07-11**\
Btw, use the common slashes and `./` for the picture's directory in the .md files (where `./` is a current directory).\
Let's discuss the formula in the code:
```Python
def cross_entropy_loss(probs, label):
    return -np.log(probs[label]+1e-10)
```
It's the natural logarithm:\
![lnx](./ml_diary_pictures/lnx.png)\
That we convert into the -ln(x):\
![minuslnx](./ml_diary_pictures/minuslnx.png)\
The 2nd graph means that if the function's input (`probs[label]`) is low, we get a higher number. If the prediction is high, we get a lower number. At x=1 y=0 (almost). It's made this way, because if the prediction is close to 1 (high), then the loss should be smaller. The input probability shouldn't be higher than 1, because the input ptobability is (ideally) the output of the softmax function, and the sum of its outputs can't be higher than 1.\
Btw, the important math fact:
```math
1e-10=10^{-10}
```
but:
```math
1^{-10}=1/{1^{10}}=1
```
so:
```math
1e-10‚â†1^{-10}
```
So don't confuse these 2.

**2025-07-12**\
A sparse vector is a vector where most of the elements are zero. For example, the `dCrossEntropyLoss/dSoftmaxProbabilites` gives us the sparse graident vector.\
The CNN with a fully-connected layer, softmax and cross-entropy loss. Painted by QWEN. I wonder how much time will pass until we look at this and wonder how neural networks could be so ineffective.\
![qwen-cnn](./ml_diary_pictures/qwen-cnn.png)

**2025-07-22**\
And interesting visualization ChatGPT wrote me during the discussion of the gradient flow during backpropagation from CE loss to Softmax output to Softmax input. I was asking how the backprop result changes, if we take different *streams* of the gradient flow. For example, what if we take the gradient flow only for the incorrect class for the $dL/dS_j$, and for all classes for the $dS_j/dzi$? In this case, according to ChatGPT (and what I understand), the result of such gradient flow is 0.\
üí° Visualization of gradient flow:
<pre>
          z0 ‚îÄ‚î¨‚îÄ‚ñ∫ s0 ‚îÄ‚îÄ‚î¨‚îÄ‚ñ∫ L
          z1 ‚îÄ‚î§        ‚îÇ
          z2 ‚îÄ‚îò        ‚ñº
                   ‚àÇL/‚àÇsy ‚â† 0
          (‚àÇL/‚àÇsj = 0 for j ‚â† y)
</pre>

**2025-08-02**\
ChatGPT told me about 3 types of convolution in CNNs, and it's kinda interesting. Especially I'm glad it's one thing that is interesting and easy to understand. Here's what it wrote.

---
### üß† Why ‚Äúvalid‚Äù convolution?

‚Äú**Valid**‚Äù doesn‚Äôt mean ‚Äúcorrect‚Äù or ‚Äúapproved by the Convolutional Council of Elders.‚Äù It‚Äôs just one of **three** standard modes of convolution padding. Let‚Äôs break them down:

#### 1. üü¢ **"valid" convolution** (used in your code)

* No padding is applied.
* Output shrinks:
  If input is $28 \times 28$, and kernel is $3 \times 3$, output becomes $26 \times 26$.
* Formula:
  $\text{output size} = \text{input size} - \text{kernel size} + 1$

#### 2. üîµ **"same" convolution**

* Adds padding so output size equals input size.
* Common in modern CNNs (like in TensorFlow/Keras).
* Useful for stacking many layers without shrinking too much.

#### 3. üü° **"full" convolution**

* Adds *maximal* padding.
* Output becomes *larger* than the input.
* Rarely used in practice unless for some signal-processing or old-school deconvolution vibes.

So no, other convolutions aren‚Äôt invalid. ‚ÄúValid‚Äù is just the Spartan version‚Äîno padding, no cheating, no mercy.

---