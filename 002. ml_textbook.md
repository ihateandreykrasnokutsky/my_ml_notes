**2025-07-09**\
It's not really a textbook in a classical sense, just a pile of information I found and *found* interesting (not necessarily it means I *understood* it though, because I like to stare at something I don't understand).\
Ok, here's the information about switch FNN (the subset of the multi-branched neural networks, that contain different kinds of neural networks to process some information). I didn't understand what is said on the picture very well, but it looks interesting.\
![switch-ffn](./ml_diary_pictures/switch-ffn.png)\
It's an MoE (mixture of experts) approach to improve efficiency and performance, the name tells what it does pretty explicitly.\

**How softmax works.**\
The code is:
```Python
def softmax(x):
    exps = np.exp(x - np.max(x))  # Numerically stable
    return exps / np.sum(exps)
```
1) It's numerically stable, because if the numbers of the input matrix (x) are high, then e^x is very high.\
`Input matrix: [50,90,100]`\
`(x-np.max(x)): [-50,-10,0]`\
But what if:\
`Input matrix: [0,0,100]`? Then\
`(x-np.max(x)): [-100,-100,0]`,\
that doesn't imporve the situation at all.\
UPD: I asked QWEN, and it said that in case of the very low numbers [-100,-100,0] we get underflow, and it's pretty good, if we don't need the *exact* small numbers. In this case we get `exps([-100,-100,0]) ‚âà [3.7e-44, 3.7e-44, 1]`, and it's practically totally fine, because it's the one-hot result.
2) In the 2nd line `exps / np.sum(exps)` makes the same array, but that sums to 1 (because of the nature of the division operation).\
To put it simply, if we have an list [1,2,3], then sum is 6, and the output list is [1/6,2/6,3/6]=[1/6,1/3,1/2], that sums up to 1, making all the output numbers effeciently the probabilites.

**2025-07-11**\
Btw, use the common slashes and `./` for the picture's directory in the .md files (where `./` is a current directory).\
Let's discuss the formula in the code:
```Python
def cross_entropy_loss(probs, label):
    return -np.log(probs[label]+1e-10)
```
It's the natural logarithm:\
![lnx](./ml_diary_pictures/lnx.png)\
That we convert into the -ln(x):\
![minuslnx](./ml_diary_pictures/minuslnx.png)\
The 2nd graph means that if the function's input (`probs[label]`) is low, we get a higher number. If the prediction is high, we get a lower number. At x=1 y=0 (almost). It's made this way, because if the prediction is close to 1 (high), then the loss should be smaller. The input probability shouldn't be higher than 1, because the input ptobability is (ideally) the output of the softmax function, and the sum of its outputs can't be higher than 1.\
Btw, the important math fact:
```math
1e-10=10^{-10}
```
but:
```math
1^{-10}=1/{1^{10}}=1
```
so:
```math
1e-10‚â†1^{-10}
```
So don't confuse these 2.

**2025-07-12**\
A sparse vector is a vector where most of the elements are zero. For example, the `dCrossEntropyLoss/dSoftmaxProbabilites` gives us the sparse graident vector.\
The CNN with a fully-connected layer, softmax and cross-entropy loss. Painted by QWEN. I wonder how much time will pass until we look at this and wonder how neural networks could be so ineffective.\
![qwen-cnn](./ml_diary_pictures/qwen-cnn.png)

**2025-07-22**\
And interesting visualization ChatGPT wrote me during the discussion of the gradient flow during backpropagation from CE loss to Softmax output to Softmax input. I was asking how the backprop result changes, if we take different *streams* of the gradient flow. For example, what if we take the gradient flow only for the incorrect class for the $dL/dS_j$, and for all classes for the $dS_j/dzi$? In this case, according to ChatGPT (and what I understand), the result of such gradient flow is 0.\
üí° Visualization of gradient flow:
<pre>
          z0 ‚îÄ‚î¨‚îÄ‚ñ∫ s0 ‚îÄ‚îÄ‚î¨‚îÄ‚ñ∫ L
          z1 ‚îÄ‚î§        ‚îÇ
          z2 ‚îÄ‚îò        ‚ñº
                   ‚àÇL/‚àÇsy ‚â† 0
          (‚àÇL/‚àÇsj = 0 for j ‚â† y)
</pre>

**2025-08-02**\
ChatGPT told me about 3 types of convolution in CNNs, and it's kinda interesting. Especially I'm glad it's one thing that is interesting and easy to understand. Here's what it wrote.

---
### üß† Why ‚Äúvalid‚Äù convolution?

‚Äú**Valid**‚Äù doesn‚Äôt mean ‚Äúcorrect‚Äù or ‚Äúapproved by the Convolutional Council of Elders.‚Äù It‚Äôs just one of **three** standard modes of convolution padding. Let‚Äôs break them down:

#### 1. üü¢ **"valid" convolution** (used in your code)

* No padding is applied.
* Output shrinks:
  If input is $28 \times 28$, and kernel is $3 \times 3$, output becomes $26 \times 26$.
* Formula:
  $\text{output size} = \text{input size} - \text{kernel size} + 1$

#### 2. üîµ **"same" convolution**

* Adds padding so output size equals input size.
* Common in modern CNNs (like in TensorFlow/Keras).
* Useful for stacking many layers without shrinking too much.

#### 3. üü° **"full" convolution**

* Adds *maximal* padding.
* Output becomes *larger* than the input.
* Rarely used in practice unless for some signal-processing or old-school deconvolution vibes.

So no, other convolutions aren‚Äôt invalid. ‚ÄúValid‚Äù is just the Spartan version‚Äîno padding, no cheating, no mercy.

---

**2025-08-05**\
ChatGPT changed the QWEN's part of the code of the first CNN I study right now.\
It said there's a bug.\
The original code:
```Python
def grad_max_pool(dpool_out, relu_out, size=2, stride=2):
    d_relu = np.zeros_like(relu_out)
    ph, pw = dpool_out.shape

    for i in range(ph):
        for j in range(pw):
            # Get the region from the ReLU output
            region = relu_out[i*stride:i*stride+size, j*stride:j*stride+size]
            max_pos = np.unravel_index(np.argmax(region), region.shape)

            # Set gradient only for the max position
            d_relu[i*stride + max_pos[0], j*stride + max_pos[1]] = dpool_out[i, j]
    
    return d_relu
```
*THe explanation*\
üß® What‚Äôs wrong with it?\
üîª 1. This line is broken:
```Python
region = np.zeros((size, size))
region_idx = np.unravel_index(np.argmax(region), region.shape)
```
It tries to find the max position inside a region‚Ä¶\
 ‚Ä¶but the region is all zeros!\
So np.argmax(region) will always return (0, 0). Meaning:\
The gradient dpool_out[i,j] always goes to the top-left corner of the pooling window,

Even if the max value wasn‚Äôt at (0,0) during the forward pass,

üíÄ That‚Äôs incorrect and will ruin training.

üîß How to fix it\
We need to pass in the original output of the ReLU, so we can find the max position during the forward pass for each pool region.\
‚úÖ Fixed version:
```Python
def grad_max_pool(dpool_out, relu_out, size=2, stride=2):
    d_relu = np.zeros_like(relu_out)
    ph, pw = dpool_out.shape

    for i in range(ph):
        for j in range(pw):
            # Get the region from the ReLU output
            region = relu_out[i*stride:i*stride+size, j*stride:j*stride+size]
            max_pos = np.unravel_index(np.argmax(region), region.shape)

            # Set gradient only for the max position
            d_relu[i*stride + max_pos[0], j*stride + max_pos[1]] = dpool_out[i, j]
    
    return d_relu
```
üîÑ And in the call site:\
You need to change the line where it's used:
```Python
dx_relu = grad_max_pool(dx_pool, relu_out.shape)
```
üîÅ Becomes:
```Python
dx_relu = grad_max_pool(dx_pool, relu_out)
```

**2025-08-29**\
I've got a funny thought. What if we add a small FC layer into each kernel? Or maybe into each application of the kernel onto the image. I don't really think about the details, just got the rough idea. It would be interesting to create such network.\
ChatGPT didn't say anything really bad about it. Basically it's much higher complexity (the exploding number of weights and  biases) and hence computationally heavy forward pass and backpropagation.

**2025-08-30**\
I changed the formula used in the max pooling, that counts the output height (or width), to make it more intuitive.\
The original formula: `out_h = (h - size) // stride + 1`\
The changed formula: `out_h = (h - size + stride) // stride`\
In the changed formula you explicitly see that we subtract the size of the pooling window from the height (height of the matrix with the map of the features) with `h - size` (because we need to do the pooling only at the tile with `last_tile_index-pooling_window_size` index, or something like this), then add the stride to see if it overshoots with `+ stride`, and then check how many times we can apply the pooling and we remove the overshooting with `// stride`.

