**2025-07-09**\
It's not really a textbook in a classical sense, just a pile of information I found and *found* interesting (not necessarily it means I *understood* it though, because I like to stare at something I don't understand).\
Ok, here's the information about switch FNN (the subset of the multi-branched neural networks, that contain different kinds of neural networks to process some information). I didn't understand what is said on the picture very well, but it looks interesting.\
![switch-ffn](/ml_diary_pictures/switch-ffn.png)\
It's an MoE (mixture of experts) approach to improve efficiency and performance, the name tells what it does pretty explicitly.\

**How softmax works.**\
The code is:
```Python
def softmax(x):
    exps = np.exp(x - np.max(x))  # Numerically stable
    return exps / np.sum(exps)
```
1) It's numerically stable, because if the numbers of the input matrix (x) are high, then e^x is very high.\
`Input matrix: [50,90,100]`\
`(x-np.max(x)): [-50,-10,0]`\
But what if:\
`Input matrix: [0,0,100]`? Then\
`(x-np.max(x)): [-100,-100,0]`,\
that doesn't imporve the situation at all.\
UPD: I asked QWEN, and it said that in case of the very low numbers [-100,-100,0] we get underflow, and it's pretty good, if we don't need the *exact* small numbers. In this case we get `exps([-100,-100,0]) â‰ˆ [3.7e-44, 3.7e-44, 1]`, and it's practically totally fine, because it's the one-hot result.
2) In the 2nd line `exps / np.sum(exps)` makes the same array, but that sums to 1 (because of the nature of the division operation).\
To put it simply, if we have an list [1,2,3], then sum is 6, and the output list is [1/6,2/6,3/6]=[1/6,1/3,1/2], that sums up to 1, making all the output numbers effeciently the probabilites.

