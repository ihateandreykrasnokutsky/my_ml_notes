**2025-07-06**\
There are 3 kinds of multiplication in the ML I know thus far:
- inner product (standard matrix multiplication, the result is a matrix or a schalar)
- outer product (like the inner product, but with vectors, the result is a matrix)
- Kronecker product (it's like the outer product of 2 matrices, the result is a matrix bigger than the previous ones)

**2025-07-07**\
Today I was discussing the psychology of ML and studying, particularly my tendency to understnad the math equations and systems through aligning my inner feelings (emotions, proprioception, etc) with how the equation/system works. ChatGPT told me it's how top scientists work with their ideas. The possible names for this are:
- embodied learning
- algorithmic identification
- systemic empathy
- becoming-the-system
The problem is that when I impersonate some system, I begin work with myself, and it can be hurtful for the ego. I need to understand that I apply my tools to something external, and apply this embodied learning more sparingly.\
I completed (rewrote from the QWENs text) the program, and understood everything (not as solid as I understand FFNNs (I don't like this abbreviation)). The most difficult part for me is recreating the derivation of the softmax function (about why exactly dz[label]-=1), the explanation of it is rather long. But I'll return to it later, I'm sure it's not so hard to understand.


**2025-07-08**\
14:08:22. Today I read the code for the backpropagation of the fully connected and conv2d layers. Also I try to use the Sublime Text, because it compiles much faster than VSC. And I try to use the Sublime Merge to upload the code to the Github.

**2025-07-09**\
Idk why it doesn't want to insert the date and time in the format dd.mm.yyyy, but fuck it. I don't really care, let it be raw.\
So yesterday I spent a lot of time (whole day), trying to make VSC to work with both .py and .cpp code. It turned out that I just put all files into one folder with the same launch.json and tasks.json file, so those files couldn't operate the compilation and debbugging of all files (considering that C++ needs compilation, and Python doesn't, plus debuggers for them are different). So in the end I put them into 2 different folders, deleted the folders .vsc (to let VSC create blank ones) => and everything started to work. Then I asked ChatGPT and QWEN for some tweaks to those files to make everthing look better.

**2025-07-11**\
Explored the natural logarithms and their role in the calculation of the Cross-Entropy Loss. I'm rather familiar with it, but repeating to improve the understanding is a good idea.

**2025-07-12**\
Now I try to understand how the chain rule is implemented into the code:
```python
def grad_fully_connected(x, weights, probs, label):
    dlogits = probs.copy()
    dlogits[label] -= 1  # derivative of CE loss w.r.t logits
    dfc_weights = np.outer(dlogits, x)
    dfc_bias = dlogits
    dx = np.dot(weights.T, dlogits)
    return dfc_weights, dfc_bias, dx
```
Tough shit, I'd say. But not more difficult than when I started to study the simple feed-forward neural networks from the scrath.\
Ok, I spent an hour and started to fall asleep, unable to dig through the QWEN's explanations further, so I call it for today.\

**2025-07-14**\
Learning the derivative of the CE loss w.r.t. softmax output. I was studying it about a week ago, I guess, so I could recall it very fast.\
After that I jumped to the derivative of the softmax w.r.t. to logits: I studied it too, but it's more complicated, and I didn't study it really thoroughly, so now it's difficult task to understand it well. But I'm moving on.\
The time of studying: <1 hour. Not very impressive, but the task is difficult too.

**2025-07-18**\
Still reading about the explanation of CE loss w.r.t. softmax logits. I read again about the derivative of the L w.r.t. softmax output. And went again to the derivative of the softmax(?) w.r.t. to logits, or the Jacobian Matrix of the softmax(?). Anyway, I read a bit of chatgpt explanations and wathced a video https://www.youtube.com/watch?v=QexBVGVM690 on the channel of Christopher Lum, called The Jacobian Matrix (I skipped the physics part, because it's late night (1:27 am), and I'm not very interested in physics (at least, this kind), though it may be helpful to see the application of the Jacobian Matrix to something, even if it's not ML).






